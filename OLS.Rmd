---
title: "OLS coefficients by hand in R and Python"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Deriving the OLS coefficients

The independent and depenendent variables in a multivariate regression can be represented in matrix notation as 
$$y = X\beta + u,$$
where 
$$X = \begin{pmatrix}x_{11}& x_{12}& \cdots & x_{1k}\\
x_{21}& x_{22}& \cdots & x_{2k}\\
\vdots  & \vdots  & \ddots & \vdots\\
x_{T1}& x_{T2}& \cdots & x_{Tk}
\end{pmatrix},
\quad
y = \begin{pmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{T}
\end{pmatrix},
\quad
u = \begin{pmatrix}u_{1}\\
u_{2}\\
\vdots\\
u_{T}
\end{pmatrix}.
$$
In matrix notation, the criterion function to be minimized is 
$$SSE(\beta) = (y - X\beta)'(y - X\beta),$$
and the first-order conditions are 
$$\frac{\partial SSE(\beta)}{\partial \beta} = -2X'(y - X\hat{\beta)} = 0,$$

which yields the normal equations, 
$$(X'X) \hat{\beta} = X'y.$$
As long as $(X'X)$ is of full rank, then 
$$\hat{\beta} = (X'X)^{-1} X'y.$$

It can be shown via the Gauss-Markov theorem that under the classical assumptions, the OLS estimator has the least variance in the class of all linear unbiased estimators of $\beta$. However, the point of this document is to show how to calculate the OLS coefficients by hand using the computer programs R and Python. Let's start with R.

## 2. Calculating $\hat{\beta}$ by hand in R

```{r}
# Number of observations
N <- 500

# Generate data for the independent variables
set.seed(4)
x0 <- runif(N, min = 1, max = 1)
x1 <- rnorm(N, 0, sd = 1)
x2 <- rnorm(N, 2, sd = 4)
x3 <- rnorm(N, -1, sd = 0.5)

# Create independent variable and define the betas
y = 2 + 5*x1 -2*x2 + 1.5*x3 + rnorm(N, 0, sd = 1)

# Convert to a data frame
df <- data.frame(x0, x1, x2, x3, y)
head(df)

# Convert data to matrix form
Y <- as.matrix(df[, "y"])
X <- as.matrix(df[, c("x0","x1","x2","x3")])

# Manually calculate OLS coefficients
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
beta_hat

# Run OLS regression and compare results
model <- lm(y ~ x1 + x2 + x3)
coef(model)

```

## 3. Calculating $\hat{\beta}$ by hand in Python

```{python}
import pandas as pd
import numpy as np
import statsmodels.api as st

# Number of observations

N = 500

# Generate data for the independent variables
np.random.seed(4)
x0 = np.ones(500)
x1 = np.random.normal(0, 1, 500)
x2 = np.random.normal(2, 4, 500)
x3 = np.random.normal(-1, 0.5, 500)

# Create independent variable and define the betas
y = 2 + 5*x1 -2*x2 + 1.5*x3 + np.random.normal(0, 1, 500)

# Create a matrix of independent variables
x = np.column_stack((x0, x1, x2, x3))

# Convert to a data frame
df = pd.DataFrame(data = x)

# Examine first 5 rows of data frame
df[:5]

# Manually calculate OLS coefficients
beta_hat = np.dot(np.linalg.inv(np.dot(x.transpose(),x)), np.dot(x.transpose(),y))
print(beta_hat)

# Run OLS regression and compare results
model = st.OLS(y, x)
results = model.fit(cov_type = 'HC1')
print(results.summary())

```

